# -*- coding: utf-8 -*-
"""kathimerini_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10AOMAqMMVQOo3FEGzDciIrshMpb9Ypto
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import numpy as np
import matplotlib.pyplot as plt
import spacy
from wordcloud import WordCloud
import seaborn as sns

"""βρίσκω τα classes και τα tags των τίτλων κλπ με βάση το url ενός άρθρου, όπως έχουμε πει στα μαθήματα"""

url = 'https://www.kathimerini.gr/politics/563660638/marinakis-gia-konstantopoyloy-threfetai-apo-arnitika-gegonota-kai-otan-den-yparchoyn-ta-dimioyrgei/'


headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


response = requests.get(url, headers=headers)


if response.status_code == 200:

    soup = BeautifulSoup(response.content, 'html.parser')


    title = soup.find("h1", {"class": "entry-title mb-5 mt-2"})
    author = soup.find("a", {"class": "url fn n"})
    datetime = soup.find("time", {"class" : "entry-date published"})

    if title:
        print("Title:", title.get_text(strip=True))
    else:
        print("Title not found.")



    if author:
      print("Author:", author.get_text(strip=True))
    else:
      print("Author <a> tag not found.")

    if datetime:
      print("datetime", datetime.get_text(strip=True))
    else:
      print("datetime not found")

    main_content = soup.find("div", {"class" : "column p-0 entry-content content"})

    for child in main_content.children:

        if child.name == 'p':
            print(child.get_text(strip=True))

        elif child.name == 'h2':
            print(child.get_text(strip=True))


else:
    print(f"Failed to retrieve the page. Status code: {response.status_code}")

def process_url(url):
    return url

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

base_url = 'https://www.kathimerini.gr/politics/'
urls = []

for page_num in range(1, 3):
    if page_num == 1:
        url = base_url
    else:
        url = f'{base_url}page/{page_num}/'

    print(f"Processing page: {url}")
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    all_links = soup.find_all("a")
    for link_tag in all_links:
        if link_tag.has_attr("href"):
            article_url = link_tag["href"]


            if (
                "/politics/" in article_url and
                "page/" not in article_url and
                "/politics/" != article_url and
                article_url.count("/") > 2 and
                article_url not in urls
            ):

                if not article_url.startswith("http"):
                    article_url = "https://www.kathimerini.gr" + article_url

                urls.append(process_url(article_url))


for u in urls:
    print(u)

"""Με τις κατάλληλες παραμέτρους τραβάω μόνο τα urls των άρθρων"""

len(urls)

import re


def process_url(url):
    return url

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

base_url = 'https://www.kathimerini.gr/politics/'
urls = []
max_pages = 34

article_pattern = re.compile(r"(https://www\.kathimerini\.gr)?/politics/\d+/")

for page_num in range(1, max_pages + 1):
    if page_num == 1:
        url = base_url
    else:
        url = f'{base_url}page/{page_num}/'

    print(f"Processing page: {url}")
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    found_new = False

    all_links = soup.find_all("a")
    for link_tag in all_links:
        if link_tag.has_attr("href"):
            article_url = link_tag["href"]

            if article_pattern.search(article_url) and article_url not in urls:
                if not article_url.startswith("http"):
                    article_url = "https://www.kathimerini.gr" + article_url

                urls.append(process_url(article_url))
                found_new = True

    if not found_new:
        print("No new articles found, stopping.")
        break

print(f"\nFound {len(urls)} article URLs:")
for u in urls:
    print(u)

len(urls)

urls = pd.DataFrame(urls)

urls = urls.rename(columns={0: "url"})

urls

import time

articles = []

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

for index, row in urls.iterrows():
    url = row["url"]
    print(f"Scraping: {url}")

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")


        title_tag = soup.find("h1", class_="entry-title mb-5 mt-2")
        title = title_tag.get_text(strip=True) if title_tag else None


        author_tag = soup.find("a", class_="url fn n")
        author = author_tag.get_text(strip=True) if author_tag else None

        date_tag = soup.find("time", class_="entry-date published")
        datetime = date_tag.get_text(strip=True) if date_tag else None


        content_tag = soup.find("div", class_="column p-0 entry-content content")
        if content_tag:
            paragraphs = []
            for child in content_tag.children:
                if child.name in ['p', 'h2']:
                    text = child.get_text(strip=True)
                    if text:
                        paragraphs.append(text)
            full_text = "\n".join(paragraphs)
        else:
            full_text = None


        articles.append({
            "url": url,
            "title": title,
            "author": author,
            "datetime": datetime,
            "text": full_text
        })

        time.sleep(1)

    except Exception as e:
        print(f"Error scraping {url}: {e}")
        articles.append({
            "url": url,
            "title": None,
            "author": None,
            "datetime": None,
            "text": None
        })

articles_df = pd.DataFrame(articles)

# Preview
print(articles_df.head())

kathimerini_df = articles_df

from google.colab import drive
drive.mount('/content/drive')

"""Αποθηκεύω το dataframe μου για να δουλέψω στη συνέχεια με αυτό"""

from google.colab import drive
drive.mount('/content/drive')


#kathimerini_df.to_csv('/content/drive/My Drive/kathimerini_df.csv', index=False)

file_path = '/content/drive/My Drive/kathimerini_df.csv'
kathimerini_df = pd.read_csv(file_path)

short = kathimerini_df[["title", "datetime"]].copy()

"""Ώρα να καθαρίσω τα δεδομένα μου. Πετάω αυτά που δε μου χρειάζονται και συμπληρώνω manually τις τιμές που μου λείπουν στη στήλη datetime. Δυστυχώς είναι αρκετές..."""

kathimerini_df = kathimerini_df.drop([333, 335, 224, 240, 265, 297, 355, 363, 377])

kathimerini_df.loc[150, "datetime"] = "06.06.2025 • 21:22"
kathimerini_df.loc[153, "datetime"] = "06.06.2025 • 15:25"
kathimerini_df.loc[165, "datetime"] = "04.06.2025 • 22:45"
kathimerini_df.loc[170, "datetime"] = "04.06.2025 • 17:13"
kathimerini_df.loc[172, "datetime"] = "04.06.2025 • 16:30"
kathimerini_df.loc[173, "datetime"] = "04.06.2025 • 13:54"
kathimerini_df.loc[178, "datetime"] = "04.06.2025 • 08:12"
kathimerini_df.loc[179, "datetime"] = "04.06.2025 • 07:45"
kathimerini_df.loc[185, "datetime"] = "03.06.2025 • 21:33"
kathimerini_df.loc[188, "datetime"] = "03.06.2025 • 18:20"
kathimerini_df.loc[190, "datetime"] = "03.06.2025 • 16:22"
kathimerini_df.loc[191, "datetime"] = "03.06.2025 • 14:49"
kathimerini_df.loc[200, "datetime"] = "03.06.2025 • 07:18"
kathimerini_df.loc[201, "datetime"] = "03.06.2025 • 07:10"
kathimerini_df.loc[205, "datetime"] = "02.06.2025 • 21:25"
kathimerini_df.loc[207, "datetime"] = "02.06.2025 • 20:37"
kathimerini_df.loc[209, "datetime"] = "02.06.2025 • 19:30"
kathimerini_df.loc[216, "datetime"] = "02.06.2025 • 16:35"
kathimerini_df.loc[217, "datetime"] = "02.06.2025 • 11:23"
kathimerini_df.loc[218, "datetime"] = "02.06.2025 • 10:20"
kathimerini_df.loc[221, "datetime"] = "02.06.2025 • 07:16"
kathimerini_df.loc[250, "datetime"] = "30.05.2025 • 07:28"
kathimerini_df.loc[266, "datetime"] = "29.05.2025 • 07:03"
kathimerini_df.loc[271, "datetime"] = "28.05.2025 • 09:11"
kathimerini_df.loc[283, "datetime"] = "27.05.2025 • 16:14"
kathimerini_df.loc[294, "datetime"] = "27.05.2025 • 07:27"
kathimerini_df.loc[295, "datetime"] = "26.05.2025 • 18:50"
kathimerini_df.loc[300, "datetime"] = "26.05.2025 • 12:04"
kathimerini_df.loc[302, "datetime"] = "26.05.2025 • 08:54"
kathimerini_df.loc[304, "datetime"] = "26.05.2025 • 07:28"
kathimerini_df.loc[307, "datetime"] = "25.05.2025 • 11:14"
kathimerini_df.loc[308, "datetime"] = "25.05.2025 • 09:05"
kathimerini_df.loc[311, "datetime"] = "24.05.2025 • 13:03"
kathimerini_df.loc[312, "datetime"] = "24.05.2025 • 12:49"
kathimerini_df.loc[316, "datetime"] = "24.05.2025 • 07:52"
kathimerini_df.loc[318, "datetime"] = "24.05.2025 • 07:36"
kathimerini_df.loc[323, "datetime"] = "23.05.2025 • 14:50"
kathimerini_df.loc[349, "datetime"] = "22.05.2025 • 07:49"
kathimerini_df.loc[353, "datetime"] = "22.05.2025 • 07:00"
kathimerini_df.loc[358, "datetime"] = "21.05.2025 • 15:47"
kathimerini_df.loc[374, "datetime"] = "20.05.2025 • 09:47"
kathimerini_df.loc[375, "datetime"] = "20.05.2025 • 07:54"
kathimerini_df.loc[376, "datetime"] = "20.05.2025 • 07:05"

short[short["datetime"].isna()]

"""ξεφορτώνομαι τoυς περιττούς χαρακτήρες από το κείμενό μου."""

import re
kathimerini_df["text"] = kathimerini_df["text"].str.replace(r"\s+", " ", regex=True).str.strip()

"""Ώρα για ανάλυση. Κάνω import τα κατάλληλα πακέτα και ξεκινάω με το vectorizing και τη spacy."""

from sklearn.feature_extraction.text import CountVectorizer

nlp = spacy.load('el_core_news_sm')

kathimerini_df_full_text = kathimerini_df['text'].str.cat(sep = ' ')

"""Επειδή έχω πολλά άρθρα, άρα και μεγάλο κείμενο πρέπει να αυξήσω το όριο των χαρακτήρων που μπορώ να επεξεργαστώ. Μετά προχωράω στο lemmatization και στη διαγραφή των stopwords για τα wordclouds."""

nlp.max_length = 10000000

kathimerini_df_full_doc = nlp(kathimerini_df_full_text)

lemmatized_text = ' '.join(token.lemma_ for token in kathimerini_df_full_doc)

stopwords = nlp.Defaults.stop_words
stopwords.add("ς")
stopwords.add("μπορώ")
stopwords.add("αναφέρω")
stopwords.add("υπάρχω")
stopwords.add("γίνομαι")
stopwords.add("ή")
stopwords.add("κάνω")
stopwords.add("θέλω")
stopwords.add("κ")
stopwords.add("λέγω")
stopwords.add("τονίζω")
stopwords.add("σημείωσε")
stopwords.add("προχωρώ")
stopwords.add("επισήμανε")

wordcloud_kathimerini_df = WordCloud(
    stopwords=nlp.Defaults.stop_words,
    width=2000,
    height=1000,
    background_color='darkgrey'
).generate(lemmatized_text)

fig = plt.figure(figsize=(40, 30), facecolor='k', edgecolor='k')
plt.imshow(wordcloud_kathimerini_df, interpolation='bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

"""Κάνω vectorizing για να φτιάξω τα διγράμματα."""

cv = CountVectorizer(stop_words= list(nlp.Defaults.stop_words), min_df=0.01, max_df=0.95)

kathimerini_df["text"] = kathimerini_df["text"].fillna("")
count_vector = cv.fit_transform(kathimerini_df["text"])

count_vector = cv.fit_transform(kathimerini_df["text"])

cv = CountVectorizer(stop_words= list(nlp.Defaults.stop_words), max_features=40, ngram_range=(2,4))
count_vector = cv.fit_transform(kathimerini_df["text"])
kathimerini_df_bigrams = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names_out())

kathimerini_df_bigrams.sum(axis =0).sort_values(ascending = False)

"""Φτιάχνω ένα custom λεξικό με τα διγράμματα που βρήκα για να συνενώσω τις ίδιες αναφορές (πχ νέα δημοκρατία και νέας δημοκρατίας), για καλύτερα αποτελέσματα."""

data = { "πρώην πρωθυπουργός" : 56,
"κυριάκος μητσοτάκης" :	56,
"αλέξη τσίπρα"	: 54,
"νέας δημοκρατίας"	: 41,
"νέας αριστεράς"	: 39,
"υποδομών μεταφορών"	: 36,
"πρώην υπουργός"	: 35,
"κράτη μέλη"	: 35,
"πλεύση ελευθερίας"	: 34,
"εκατ ευρώ"	: 33,
"απε μπε"	: 33,
"κυριάκο μητσοτάκη"	: 33,
"πρώην υπουργό"	: 32,
"πλεύσης ελευθερίας" :	30,
"νέα αριστερά"	:30,
"μονή σινά" :	29,
"υπουργός εξωτερικών"	:29,
"νέα δημοκρατία" :	29,
"πρωθυπουργός κυριάκος" :	29,
"σύμφωνα πληροφορίες"	: 28,
"εθνικής αμυνας" : 28,
"αλέξης τσίπρας"	: 28,
"σύσταση προανακριτικής"	: 28,
"πρώην πρωθυπουργού"	: 28,
"αγροτικής ανάπτυξης"	: 26,
"πρωθυπουργός κυριάκος μητσοτάκης"	: 25,
"πρόεδρος συριζα"	: 25,
"αγίας αικατερίνης"	: 25,
"χαριλάου τρικούπη"	: 24,
"μέση ανατολή"	: 23,
"προανακριτικής επιτροπής" : 23,
"casus belli"	: 22,
"πηγή απε"	: 22,
"μονής σινά"	: 22,
"εξωτερική πολιτική" :	22,
"κυβερνητικός εκπρόσωπος" : 22}

data["Νέα Δημοκρατία/Νέας Δημοκρατίας"] = data.pop("νέας δημοκρατίας") + data.pop("νέα δημοκρατία")
data['Κυριάκος Μητσοτάκης/Κυριάκου Μητσοτάκη/Κυριάκο Μητσοτάκη'] = data.pop('κυριάκος μητσοτάκης') + data.pop('κυριάκο μητσοτάκη')
data['Αλέξης Τσίπρας/Αλέξη Τσίπρα'] = data.pop('αλέξης τσίπρας') + data.pop('αλέξη τσίπρα')
data['Πλεύση Ελυθερίας/Πλεύσης Ελευθερίας'] = data.pop('πλεύσης ελευθερίας') + data.pop('πλεύση ελευθερίας')
data['Νέα Αριστερά/Νέας Αριστεράς'] = data.pop('νέα αριστερά') + data.pop('νέας αριστεράς')

df = pd.DataFrame(list(data.items()), columns=['Διγράμματα', 'Αναφορές'])

df = df.sort_values(by='Αναφορές', ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(df['Διγράμματα'], df['Αναφορές'], color='darkgrey', edgecolor= 'black')
bars = plt.barh(df['Διγράμματα'], df['Αναφορές'], color='darkgrey', edgecolor= 'black')
for bar in bars:
    plt.text(
        bar.get_width() + 2, bar.get_y() + bar.get_height() / 2,
        str(bar.get_width()),
        va='center',
        ha='left',
        color='red',
        fontsize=10
    )

plt.xlabel('Αναφορές')
plt.ylabel('Διγράμματα')
plt.xlim(0,120)
plt.title('Διγράμματα Αναφορών kathimerini.gr, Μάιος - Ιούνιος 2025')
plt.gca().invert_yaxis()
plt.show()

"""Διαχωρίζω το datetime για να κάνω ανάλυση των άρθρων ανά μέρα."""

kathimerini_df[["date", "time"]] = kathimerini_df["datetime"].str.split(" • ", expand=True)

datetime_counts = kathimerini_df['date'].value_counts().sort_index()


fig, ax = plt.subplots(figsize=(14, 6))
bars = ax.bar(datetime_counts.index.astype(str), datetime_counts.values, color='yellow', edgecolor='black')

# Titles and labels
ax.set_title("Kathimerini.gr - Δημοσιευμένα Άρθρα ανά μέρα", fontsize=16, weight='bold')
ax.set_xlabel("Ημερομηνίες", fontsize=12)
ax.set_ylabel("Άρθρα", fontsize=12)

# Set Y-axis limit
ax.set_ylim(0, 25)

# Add values above bars
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, height + 0.5, str(height), ha='center', va='bottom', fontsize=9)

# Beautify ticks
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)
ax.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

datetime_counts_table = (
    kathimerini_df['date']
    .value_counts()
    .rename_axis('date')
    .reset_index(name='article_count')
    .sort_values(by='article_count', ascending=False)
)

# Display the table
datetime_counts_table.head(10)

"""Περνάω στην ανάλυση συναισθημάτων. Κάνω import to emolex και όσα έχουμε μάθει για να μετρήσω το polarity (θετικές λέξεις μείον αρνητικές). Μετά συνδυάζω την πολικότητα με τη στήλη της ημερομηνίας για να βρω την πολικότητα ανά μέρα."""

filepath2 = "https://raw.githubusercontent.com/datajour-gr/DataJournalism/main/Bachelor%20Lessons%202023/Lesson%2010/NRC_GREEK_Translated_6_2020.csv"

emolex_df = pd.read_csv(filepath2)

emolex_df = emolex_df.drop_duplicates(subset=['word'])
emolex_df = emolex_df.dropna()
emolex_df.reset_index(inplace = True, drop=True)

vec = CountVectorizer(analyzer = 'word', vocabulary = emolex_df.word,
                      lowercase=False,
                      strip_accents = 'unicode',
                      stop_words= list(nlp.Defaults.stop_words),
                      ngram_range=(1, 2))

clean_texts = kathimerini_df["text"].dropna()

matrix = vec.fit_transform(clean_texts)
vocab = vec.get_feature_names_out()
wordcount_df = pd.DataFrame(matrix.toarray(), columns=vocab)

#clean_texts

positive_words = emolex_df[emolex_df.Positive == 1]['word']

negative_words = emolex_df[emolex_df.Negative == 1]['word']

kathimerini_df['positive_text'] = wordcount_df[positive_words].sum(axis=1)

kathimerini_df['negative_text'] = wordcount_df[negative_words].sum(axis=1)

kathimerini_df['pos/neg_text'] = kathimerini_df['positive_text'] - kathimerini_df['negative_text']

float(kathimerini_df['pos/neg_text'].mean().round(2))

kathimerini_df_copy = kathimerini_df

kathimerini_df_copy['datetime'] = kathimerini_df_copy['datetime'].astype(str)


kathimerini_df_copy[['date', 'time']] = kathimerini_df_copy['datetime'].str.split(' • ', expand=True)

#kathimerini_df_copy.head()

kathimerini_df_copy['date'] = pd.to_datetime(kathimerini_df_copy['date'], format="%d.%m.%Y")

# Set datetime as index
kathimerini_df_copy.set_index('date', inplace=True)

daily_sentiment = kathimerini_df_copy['pos/neg_text'].resample('D').mean()

colors = daily_sentiment.apply(
    lambda x: 'green' if x > 0 else 'red' if x < 0 else 'black'
)


plt.figure(figsize=(16, 6))
plt.scatter(daily_sentiment.index, daily_sentiment.values,
            c=colors,
            edgecolors='black',
            s=70)


plt.plot(daily_sentiment.index, daily_sentiment.values,
         linestyle='-', color='gray', linewidth=1)


plt.title("Ημερήσια Μέση Θετικότητα/Αρνητικότητα Άρθρων - kathimerini.gr", fontsize=15, weight='bold')
plt.xlabel("Ημερομηνία", fontsize=12)
plt.ylabel("Μέσο Σκορ Sentiment", fontsize=12)
plt.ylim(-2, 15)
plt.xticks(rotation=90)
plt.grid(axis='y', linestyle='--', alpha=0.6)


plt.xticks(ticks=daily_sentiment.index, labels=[d.strftime('%d-%m-%Y') for d in daily_sentiment.index])

plt.ti