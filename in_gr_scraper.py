# -*- coding: utf-8 -*-
"""in.gr scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuHf-nALPLc82jVkAzfb9P99Y1fbCte7

Κάνω Import τις περισσότερες από τις βιβλιοθήκες που θα μου χρειαστούν στην εργασία
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd3
import time
import numpy as np
import matplotlib.pyplot as plt
import spacy
from wordcloud import WordCloud
import seaborn as sns

"""Βρίσκω τα classes και τα tags των τίτλων κλπ με βάση το url ενός άρθρου, όπως έχουμε πει στα μαθήματα"""

url = 'https://www.in.gr/2025/06/20/politics/politiki-grammateia/tsoukalas-nd-paragei-pio-polla-skandala-ap-osa-antexei-xora-ti-fanela-tis-diafthoras-paei-stin-eyropi/'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


response = requests.get(url, headers=headers)


if response.status_code == 200:

    soup = BeautifulSoup(response.content, 'html.parser')

    title = soup.find("h1", {"class": "is-semibold is-sans-serif-font is-darkblue article-headline"})
    author = soup.find("a", {"class" : "blue-c vcard author" })
    datetime = soup.find("div", {"class" : "is-sans-serif-font"})
    main_content = soup.find("div", {"class" : "inner-main-article"})
    if title:
        print("Title:", title.get_text(strip=True))

    else:
        print("Title not found.")

    if author:
      print("Author:", author.get_text(strip=True))
    else:
      print("Author <a> tag not found.")

    if datetime:
      print("Datetime:", datetime.get_text(strip=True))
    else:
      print("Datetime not found.")


    for child in main_content.children:

        if child.name == 'p':
            print(child.get_text(strip=True))

        elif child.name == 'h2':
            print(child.get_text(strip=True))


else:
    print(f"Failed to retrieve the page. Status code: {response.status_code}")

"""Πάλι αντιμετωπίζω πρόβλημα γιατί μου τραβάει και σελίδες. Βρίσκω τρόπο να το ξεφορτωθώ."""

def process_url(url):
    return url

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

base_url = 'https://www.in.gr/politics/'
urls = []

# List of unwanted paths (relative)
excluded_paths = [
    '/politics/',
    '/politics/page/',
    '/politics/diplomatia/',
    '/politics/politiki-grammateia/',
    '/politics/paraskinio/',
    '/politics/aftodioikisi/',
    '/politics/in-confidential/',
]

def is_valid_article_url(url):

    if not url.startswith("http"):
        url = "https://www.in.gr" + url


    for excluded in excluded_paths:
        if url.startswith("https://www.in.gr" + excluded):
            return False


    return "/politics/" in url and url.count("/") > 4

for page_num in range(1,28):
    url = base_url if page_num == 1 else f'{base_url}page/{page_num}/'
    print(f"Processing page: {url}")

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    all_links = soup.find_all("a", href=True)
    for link_tag in all_links:
        href = link_tag["href"]
        if is_valid_article_url(href):
            full_url = href if href.startswith("http") else "https://www.in.gr" + href
            if full_url not in urls:
                urls.append(process_url(full_url))

# Print collected URLs
for u in urls:
    print(u)

urls = pd.DataFrame(urls)

urls = urls.rename(columns={0: "url"})

"""Κάνω τον κώδικα όσο πιο καθαρό μπορώ για να μου τραβήξει με πιο αποτελεσματικό τρόπο τα δεδομένα. Οι μεταβλητές tags μου μειώνουν τις πιθανότητες να μην μου φέρει καθαρά αυτά που ζητάω."""

import time

articles = []

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

for index, row in urls.iterrows():
    url = row["url"]
    print(f"Scraping: {url}")

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")


        title_tag = soup.find("h1", {"class": "is-semibold is-sans-serif-font is-darkblue article-headline"})
        title = title_tag.get_text(strip=True) if title_tag else None


        author_tag = soup.find("a", {"class" : "blue-c vcard author" })
        author = author_tag.get_text(strip=True) if author_tag else None

        date_tag = soup.find("div", {"class" : "is-sans-serif-font"})
        datetime = date_tag.get_text(strip=True) if date_tag else None


        content_tag = soup.find("div", {"class" : "inner-main-article"})
        if content_tag:
            paragraphs = []
            for child in content_tag.children:
                if child.name in ['p', 'h2']:
                    text = child.get_text(strip=True)
                    if text:
                        paragraphs.append(text)
            full_text = "\n".join(paragraphs)
        else:
            full_text = None


        articles.append({
            "url": url,
            "title": title,
            "author": author,
            "datetime": datetime,
            "text": full_text
        })

        time.sleep(1.5)

    except Exception as e:
        print(f"Error scraping {url}: {e}")
        articles.append({
            "url": url,
            "title": None,
            "author": None,
            "datetime": None,
            "text": None
        })

articles_df = pd.DataFrame(articles)

# Preview
print(articles_df.head())

in_gr_articles = articles_df

from google.colab import drive
drive.mount('/content/drive')
#in_gr_articles.to_csv('/content/drive/My Drive/in_gr_articles.csv', index=False)

file_path = '/content/drive/My Drive/in_gr_articles.csv'
in_gr_articles = pd.read_csv(file_path)

pd.set_option('display.max_rows', 1190)

from datetime import datetime as dt
import re
import json

articles = []

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

def clean_and_parse_greek_date(raw_date):

    raw_date = re.sub(r'([Α-Ωα-ωάέήίόύώϊϋΐΰ])(\d)', r'\1 \2', raw_date)
    raw_date = " ".join(raw_date.split())
    try:
        return dt.strptime(raw_date, "%A %d %B %Y")
    except ValueError:
        return None

def extract_date_from_ld_json(soup):
    for tag in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(tag.string)
            if isinstance(data, dict) and "datePublished" in data:
                return dt.fromisoformat(data["datePublished"].split("T")[0])
        except:
            continue
    return None

for index, row in urls.iterrows():
    url = row["url"]
    print(f"Scraping: {url}")

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")

        # Try <div class="article__date"> first
        date_tag = soup.find("div", class_="article__date")
        if date_tag:
            raw_date = date_tag.get_text(strip=True)
            parsed_date = clean_and_parse_greek_date(raw_date)
        else:

            parsed_date = extract_date_from_ld_json(soup)

        articles.append({
            "url": url,
            "datetime": parsed_date
        })

    except Exception as e:
        print(f"Error scraping {url}: {e}")
        articles.append({
            "url": url,
            "datetime": None
        })

articles_df = pd.DataFrame(articles)
print(articles_df.head())

urls = pd.DataFrame(urls)

"""Δεν μου τραβάει σωστά την ημερομηνία οπότε πρέπει να τις περάσω manually..."""

merged_df = in_gr_articles.merge(
    articles_df[['url', 'datetime']],
    on='url',
    how='left'
)


merged_df = merged_df.rename(columns={'datetime': 'date'})


in_gr_articles = merged_df

print(in_gr_articles.head())

merged_df = merged_df.drop(columns=['datetime_x'])

filepath = '/content/drive/MyDrive/in_gr_articles.csv'
in_gr_articles.to_csv(filepath, index=False)
print(f"Saved to {filepath}")

filepath = '/content/drive/MyDrive/in_gr_articles.csv'
in_gr_articles = pd.read_csv(filepath)

"""Η προσθήκη της σωστής ημερομηνίας γίνεται με βάση το url και χειροκίνητα..."""

in_gr_articles["text"] = in_gr_articles["text"].str.replace(r"\s+", " ", regex=True).str.strip()

in_gr_articles.loc[ in_gr_articles['url'] == 'https://www.in.gr/2025/05/28/politics/syriza-kyvernisi-mitsotaki-einai-o-fysikos-kai-ithikos-aytourgos-gia-tin-katarreysi-tou-opekepe/', 'datetime_y' ] = pd.Timestamp('2025-05-21')

in_gr_articles[in_gr_articles['datetime_y'].isna()]

in_gr_articles = in_gr_articles.drop(index=492)

in_gr_articles = pd.read_csv(filepath)

in_gr_articles.rename(columns={

    'datetime_y': 'datetime'
}, inplace=True)

in_gr_articles.loc[542:546, 'datetime'] = pd.to_datetime('2025-05-22').date()

in_gr_articles.loc[465, 'datetime'] = pd.to_datetime('2025-05-26').date()

"""Κάνω διανυσματοποίηση και κάνω install το πακέτο της spacy"""

from sklearn.feature_extraction.text import CountVectorizer

!python -m spacy download el_core_news_sm

nlp = spacy.load('el_core_news_sm')

in_gr_articles_full_text = in_gr_articles['text'].str.cat(sep = ' ')

"""Επειδή έχω πολλά άρθρα, άρα και μεγάλο κείμενο πρέπει να αυξήσω το όριο των χαρακτήρων που μπορώ να επεξεργαστώ. Μετά προχωράω στο lemmatization και στη διαγραφή των stopwords για τα wordclouds."""

nlp.max_length = 10000000

in_gr_articles_full_doc = nlp(in_gr_articles_full_text)

lemmatized_text = ' '.join(token.lemma_ for token in in_gr_articles_full_doc)

stopwords = nlp.Defaults.stop_words
stopwords.add("ς")
stopwords.add("μπορώ")
stopwords.add("αναφέρω")
stopwords.add("υπάρχω")
stopwords.add("γίνομαι")
stopwords.add("ή")
stopwords.add("κάνω")
stopwords.add("θέλω")
stopwords.add("κ")
stopwords.add("λέγω")
stopwords.add("τονίζω")
stopwords.add("σημείωσε")
stopwords.add("προχωρώ")
stopwords.add("επισήμανε")

wordcloud_in_gr_articles = WordCloud(
    stopwords=nlp.Defaults.stop_words,
    width=2000,
    height=1000,
    background_color='black'
).generate(lemmatized_text)

fig = plt.figure(figsize=(40, 30), facecolor='k', edgecolor='k')
plt.imshow(wordcloud_in_gr_articles, interpolation='bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

"""Κάνω vectorizing για να φτιάξω τα διγράμματα."""

cv = CountVectorizer(stop_words= list(nlp.Defaults.stop_words), min_df=0.01, max_df=0.95)

in_gr_articles["text"] = in_gr_articles["text"].fillna("")
count_vector = cv.fit_transform(in_gr_articles["text"])

count_vector = cv.fit_transform(in_gr_articles["text"])

cv = CountVectorizer(stop_words= list(nlp.Defaults.stop_words), max_features=40, ngram_range=(2,2))
count_vector = cv.fit_transform(in_gr_articles["text"])
in_gr_articles_bigrams = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names_out())

in_gr_articles_bigrams.sum(axis =0).sort_values(ascending = False)

"""Φτιάχνω ένα custom λεξικό με τα διγράμματα που βρήκα για να συνενώσω τις ίδιες αναφορές (πχ νέα δημοκρατία και νέας δημοκρατίας), για καλύτερα αποτελέσματα."""

data = { "συριζα πς" :	224,
"νέας δημοκρατίας"	: 154,
"νέα δημοκρατία"	: 152,
"πασοκ κιναλ" :	145,
"κυριάκος μητσοτάκης" :	134,
"νέα αριστερά"	: 109,
"αλέξη τσίπρα"	:108,
"σωκράτης φάμελλος"	: 98,
"προανακριτικής επιτροπής" :	91,
"νέας αριστεράς"	: 89,
"μέση ανατολή"	: 82,
"προοδευτική συμμαχία"	: 81,
"σύσταση προανακριτικής"	: 81,
"κυριάκο μητσοτάκη"	: 76,
"νίκος ανδρουλάκης"	: 74,
" κινήματος αλλαγής"	: 74,
"εκπρόσωπος τύπου"	:73,
"ελληνική κυβέρνηση"	:67,
"αλέξης τσίπρας"	: 67,
"κυβερνητικός εκπρόσωπος" :	62,
"κυριάκου μητσοτάκη"	:61,
"πρόεδρος συριζα" :	60,
"εξωτερική πολιτική"	: 60,
"blue skies"	: 59,
"σκάνδαλο οπεκεπε" :	57,
"πρόεδρος πασοκ"	: 56,
"μέλος επανεκκίνησης"	: 53,
"έλληνες πολίτες"	: 53,
"ανθρωπιστικής βοήθειας"	: 53,
"υπουργός εξωτερικών"	:49,
"υπουργείο εξωτερικών"	:49,
"συμβουλίου ασφαλείας" :	49,
"παύλος μαρινάκης"	:49,
"σύμφωνα πληροφορίες":	48,
"αγίας αικατερίνης"	:48,
"χαριλάου τρικούπη" :	48,
"διεθνές δίκαιο"	: 47}

data["Νέα Δημοκρατία/Νέας Δημοκρατίας"] = data.pop("νέας δημοκρατίας") + data.pop("νέα δημοκρατία")
data['Κυριάκος Μητσοτάκης/Κυριάκου Μητσοτάκη/Κυριάκο Μητσοτάκη'] = data.pop('κυριάκος μητσοτάκης')  + data.pop('κυριάκου μητσοτάκη') + data.pop('κυριάκο μητσοτάκη')
data['Αλέξης Τσίπρας/Αλέξη Τσίπρα'] = data.pop('αλέξης τσίπρας') + data.pop('αλέξη τσίπρα')

df = pd.DataFrame(list(data.items()), columns=['Διγράμματα', 'Αναφορές'])

df = df.sort_values(by='Αναφορές', ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(df['Διγράμματα'], df['Αναφορές'], color='blue', edgecolor= 'black')
bars = plt.barh(df['Διγράμματα'], df['Αναφορές'], color='blue', edgecolor= 'black')
for bar in bars:
    plt.text(
        bar.get_width() + 2, bar.get_y() + bar.get_height() / 2,
        str(bar.get_width()),
        va='center',
        ha='left',
        color='red',
        fontsize=10
    )

plt.xlabel('Αναφορές')
plt.ylabel('Διγράμματα')
plt.xlim(0,400)
plt.title('Διγράμματα Αναφορών in.gr, Μάιος - Ιούνιος 2025')
plt.gca().invert_yaxis()
plt.show()

"""κάνω ανάλυση των άρθρων ανά μέρα με βάση την ημερομηνία. Το in.gr έχει μόνο ημερομηνία και όχι ώρα, οπότε δεν χρειάζεται να τα διαχωρίσω."""

datetime_counts = in_gr_articles['datetime'].value_counts().sort_index()


print(datetime_counts.sort_values(ascending=False).head(10))

plt.figure(figsize=(12,6))
datetime_counts.plot(kind='bar')
plt.title("Number of Articles Published per Day")
plt.xlabel("Date")
plt.ylabel("Article Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

datetime_counts_table = (
    in_gr_articles['datetime']
    .value_counts()
    .rename_axis('datetime')
    .reset_index(name='article_count')
    .sort_values(by='article_count', ascending=False)
)

# Display the table
datetime_counts_table.head(10)

datetime_counts = in_gr_articles['datetime'].value_counts().sort_index()


fig, ax = plt.subplots(figsize=(14, 6))
bars = ax.bar(datetime_counts.index.astype(str), datetime_counts.values, color='red', edgecolor='black')


ax.set_title("In.gr - Δημοσιευμένα Άρθρα ανά μέρα", fontsize=16, weight='bold')
ax.set_xlabel("Ημερομηνίες", fontsize=12)
ax.set_ylabel("Άρθρα", fontsize=12)


ax.set_ylim(0, 50)


for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, height + 0.5, str(height), ha='center', va='bottom', fontsize=9)


plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)
ax.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

"""Περνάω στην ανάλυση συναισθημάτων. Κάνω import to emolex και όσα έχουμε μάθει για να μετρήσω το polarity (θετικές λέξεις - αρνητικές). Μετά συνδυάζω την πολικότητα με τη στήλη της ημερομηνίας για να βρω την πολικότητα ανά μέρα."""

filepath2 = "https://raw.githubusercontent.com/datajour-gr/DataJournalism/main/Bachelor%20Lessons%202023/Lesson%2010/NRC_GREEK_Translated_6_2020.csv"

emolex_df = pd.read_csv(filepath2)

emolex_df = emolex_df.drop_duplicates(subset=['word'])
emolex_df = emolex_df.dropna()
emolex_df.reset_index(inplace = True, drop=True)

vec = CountVectorizer(analyzer = 'word', vocabulary = emolex_df.word,
                      lowercase=False,
                      strip_accents = 'unicode',
                      stop_words= list(nlp.Defaults.stop_words),
                      ngram_range=(1, 2))

clean_texts = in_gr_articles["text"].dropna()

matrix = vec.fit_transform(clean_texts)
vocab = vec.get_feature_names_out()
wordcount_df = pd.DataFrame(matrix.toarray(), columns=vocab)

#clean_texts

positive_words = emolex_df[emolex_df.Positive == 1]['word']

negative_words = emolex_df[emolex_df.Negative == 1]['word']

in_gr_articles['positive_text'] = wordcount_df[positive_words].sum(axis=1)

in_gr_articles['negative_text'] = wordcount_df[negative_words].sum(axis=1)

in_gr_articles['pos/neg_text'] = in_gr_articles['positive_text'] - in_gr_articles['negative_text']

float(in_gr_articles['pos/neg_text'].mean().round(2))

in_gr_articles_copy = in_gr_articles

in_gr_articles_copy['datetime'] = pd.to_datetime(in_gr_articles_copy['datetime'])

in_gr_articles_copy.set_index('datetime', inplace=True)

daily_sentiment = in_gr_articles_copy['pos/neg_text'].resample('D').mean()

colors = daily_sentiment.apply(
    lambda x: 'green' if x > 0 else 'red' if x < 0 else 'black'
)

plt.figure(figsize=(16, 6))
plt.scatter(daily_sentiment.index, daily_sentiment.values,
            c=colors,
            edgecolors='black',
            s=70)

plt.plot(daily_sentiment.index, daily_sentiment.values,
         linestyle='-', color='gray', linewidth=1)

plt.title("Ημερήσια Μέση Θετικότητα/Αρνητικότητα Άρθρων - in.gr", fontsize=15, weight='bold')
plt.xlabel("Ημερομηνία", fontsize=12)
plt.ylabel("Polarity", fontsize=12)
plt.ylim(-4, 12)
plt.xticks(rotation=90)
plt.grid(axis='y', linestyle='--', alpha=0.6)

plt.xticks(ticks=daily_sentiment.index, labels=[d.strftime('%d-%m-%Y') for d in daily_sentiment.index])

plt.tight_layout()
plt.show()